{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dimensions, act_dimensions):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dimensions, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, act_dimensions),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def get_distribution(self, obs):\n",
    "        return Categorical(logits=self.actor(obs))\n",
    "    \n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions\n",
    "        pi = self.get_distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a\n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dimensions):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dimensions, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.critic(obs), -1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_space, act_space):\n",
    "        super().__init__()\n",
    "        \n",
    "        obs_dimensions = obs_space.shape[0]\n",
    "        \n",
    "        # policy network\n",
    "        self.pi = Actor(obs_dimensions, act_space.n)\n",
    "        \n",
    "        # value network\n",
    "        self.v = Critic(obs_dimensions)\n",
    "        \n",
    "        \n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi.get_distribution(obs)\n",
    "            action = pi.sample()\n",
    "            logp_a = pi.log_prob(action)\n",
    "            val = self.v(obs)\n",
    "\n",
    "        return action.numpy(), val.numpy(), logp_a.numpy()\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        return self.step(obs)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions\n",
    "num_workers = 1\n",
    "pi_lr = 3e-4\n",
    "v_lr = 1e-3\n",
    "epochs = 50 # 50\n",
    "steps_per_epoch = 4000 // num_workers\n",
    "max_ep_len = 1000\n",
    "train_pi_iters = 80\n",
    "train_v_iters = 80\n",
    "gamma = 0.99\n",
    "lam = 0.97\n",
    "clip_ratio_offset = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    return signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo():\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    obs_space = env.observation_space\n",
    "    act_space = env.action_space\n",
    "\n",
    "    actor_critic = ActorCritic(obs_space, act_space)\n",
    "    \n",
    "    obs_buf  = np.zeros((steps_per_epoch, obs_space.shape[0]), dtype=np.float32)\n",
    "    act_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    adv_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    rew_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    rew_boot_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    val_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    logp_buf = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    \n",
    "    pi_optim = Adam(actor_critic.pi.parameters(), lr=pi_lr)\n",
    "    v_optim = Adam(actor_critic.v.parameters(), lr=v_lr)\n",
    "    \n",
    "    def ppo_pi_loss(data):\n",
    "        obs, action, advantage, old_logp = data['obs'], data['act'], data['adv'], data['logp']\n",
    "        \n",
    "        pi, logp = actor_critic.pi(obs, action)\n",
    "        pi_ratio = torch.exp(logp - old_logp)\n",
    "        clipped_adv = torch.clamp(pi_ratio, 1 - clip_ratio_offset, 1 + clip_ratio_offset) * advantage\n",
    "        pi_loss = -(torch.min(pi_ratio * advantage, clipped_adv)).mean()\n",
    "        return pi_loss\n",
    "    \n",
    "    def ppo_v_loss(data):\n",
    "        obs, adj_rewards = data['obs'], data['rew']\n",
    "        return (actor_critic.v(obs) - adj_rewards).pow(2).mean()\n",
    "    \n",
    "    def ppo_update():\n",
    "        data = {k: torch.as_tensor(v, dtype=torch.float32)\n",
    "                for k, v in dict(obs=obs_buf, act=act_buf, rew=rew_boot_buf, adv=adv_buf, logp=logp_buf).items()}\n",
    "        \n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optim.zero_grad()\n",
    "            pi_loss = ppo_pi_loss(data)\n",
    "            pi_loss.backward()\n",
    "            pi_optim.step()\n",
    "            \n",
    "        for i in range(train_v_iters):\n",
    "            v_optim.zero_grad()\n",
    "            v_loss = ppo_v_loss(data)\n",
    "            v_loss.backward()\n",
    "            v_optim.step()\n",
    "\n",
    "    obs, ep_reward, ep_len = env.reset(), 0, 0\n",
    "    ep_reward_history = np.zeros(epochs, dtype=np.float32)\n",
    "    for ep in range(epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            action, val, logp_a = actor_critic.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            \n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_len += 1\n",
    "            \n",
    "            # buf.store(obs, action, reward, val, logp_a)\n",
    "            obs_buf[t] = obs\n",
    "            act_buf[t] = action\n",
    "            rew_buf[t] = reward\n",
    "            val_buf[t] = val\n",
    "            logp_buf[t] = logp_a\n",
    "            \n",
    "            obs = new_obs\n",
    "            \n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = done or timeout\n",
    "            epoch_ended = t==steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if timeout or epoch_ended:\n",
    "                    _, bootstrap_v, _ = actor_critic.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "                else:\n",
    "                    bootstrap_v = 0\n",
    "\n",
    "                rews_aug = np.append(rew_buf[:], bootstrap_v)\n",
    "                vals_aug = np.append(val_buf[:], bootstrap_v)\n",
    "                \n",
    "                # GAE-Lambda advantage calculation\n",
    "                gae_deltas = rews_aug[:-1] + gamma * vals_aug[1:] - vals_aug[:-1]\n",
    "                adv_buf[:] = discount_cumsum(gae_deltas, gamma * lam)\n",
    "        \n",
    "                # Computes rewards-to-go, to be targets for the value function\n",
    "                rew_boot_buf[:] = discount_cumsum(rews_aug, gamma)[:-1]\n",
    "\n",
    "                ep_reward_history[ep] = max(ep_reward_history[ep], ep_reward)\n",
    "                obs, ep_reward, done = env.reset(), 0, 0\n",
    "        \n",
    "        ppo_update()\n",
    "        print(f\"epoch: {ep + 1}, max reward: {ep_reward_history[ep]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, max reward: 10.138\n",
      "epoch: 2, max reward: 0.000\n",
      "epoch: 3, max reward: 0.000\n",
      "epoch: 4, max reward: 0.000\n",
      "epoch: 5, max reward: 0.655\n",
      "epoch: 6, max reward: 60.277\n",
      "epoch: 7, max reward: 0.000\n",
      "epoch: 8, max reward: 0.000\n",
      "epoch: 9, max reward: 23.299\n",
      "epoch: 10, max reward: 25.900\n",
      "epoch: 11, max reward: 111.352\n",
      "epoch: 12, max reward: 3.798\n",
      "epoch: 13, max reward: 0.907\n",
      "epoch: 14, max reward: 34.417\n",
      "epoch: 15, max reward: 94.119\n",
      "epoch: 16, max reward: 64.678\n",
      "epoch: 17, max reward: 4.103\n",
      "epoch: 18, max reward: 25.965\n",
      "epoch: 19, max reward: 41.973\n",
      "epoch: 20, max reward: 46.301\n",
      "epoch: 21, max reward: 27.627\n",
      "epoch: 22, max reward: 120.285\n",
      "epoch: 23, max reward: 139.240\n",
      "epoch: 24, max reward: 82.115\n",
      "epoch: 25, max reward: 74.720\n",
      "epoch: 26, max reward: 0.000\n",
      "epoch: 27, max reward: 45.128\n",
      "epoch: 28, max reward: 55.603\n",
      "epoch: 29, max reward: 60.568\n",
      "epoch: 30, max reward: 96.719\n",
      "epoch: 31, max reward: 75.391\n",
      "epoch: 32, max reward: 126.944\n",
      "epoch: 33, max reward: 47.701\n",
      "epoch: 34, max reward: 108.469\n",
      "epoch: 35, max reward: 94.642\n",
      "epoch: 36, max reward: 111.691\n",
      "epoch: 37, max reward: 83.553\n",
      "epoch: 38, max reward: 136.031\n",
      "epoch: 39, max reward: 70.689\n",
      "epoch: 40, max reward: 149.846\n",
      "epoch: 41, max reward: 78.895\n",
      "epoch: 42, max reward: 114.932\n",
      "epoch: 43, max reward: 150.439\n",
      "epoch: 44, max reward: 13.387\n",
      "epoch: 45, max reward: 78.390\n",
      "epoch: 46, max reward: 133.679\n",
      "epoch: 47, max reward: 54.158\n",
      "epoch: 48, max reward: 65.179\n",
      "epoch: 49, max reward: 132.501\n",
      "epoch: 50, max reward: 157.283\n"
     ]
    }
   ],
   "source": [
    "ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
