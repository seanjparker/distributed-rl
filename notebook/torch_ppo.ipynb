{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.optim import Adam\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dimensions, act_dimensions):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dimensions, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, act_dimensions)\n",
    "        )\n",
    "        \n",
    "    def get_distribution(self, obs):\n",
    "        return Categorical(logits=self.actor(obs))\n",
    "    \n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions\n",
    "        pi = self.get_distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a\n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dimensions):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dimensions, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.critic(obs), -1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_space, act_space):\n",
    "        super().__init__()\n",
    "        \n",
    "        obs_dimensions = obs_space.shape[0]\n",
    "        \n",
    "        # policy network\n",
    "        self.pi = Actor(obs_dimensions, act_space.n)\n",
    "        \n",
    "        # value network\n",
    "        self.v = Critic(obs_dimensions)\n",
    "        \n",
    "        \n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi.get_distribution(obs)\n",
    "            action = pi.sample()\n",
    "            logp_a = pi.log_prob(action)\n",
    "            \n",
    "            val = self.v(obs)\n",
    "\n",
    "        return action.numpy(), val.numpy(), logp_a.numpy()\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        return self.step(obs)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions\n",
    "pi_lr = 3e-4\n",
    "vf_lr = 1e-3\n",
    "epochs = 10 # 50\n",
    "steps_per_epoch = 100 # 4000\n",
    "max_ep_len=1000\n",
    "train_pi_iters = 80\n",
    "train_v_iters = 80\n",
    "gamma = 0.99\n",
    "lam = 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalized Advantage Estimation\n",
    "def gae(rewards, values):\n",
    "    deltas = rewards[:-1] + gamma * values[:-1] - values[:-1]\n",
    "\n",
    "    # RLlab -- https://github.com/rll/rllab/blob/ba78e4c16dc492982e648f117875b22af3965579/rllab/misc/special.py#L107\n",
    "    # Computing discounted cumulative sums of vectors\n",
    "    return scipy.signal.lfilter([1], [1, float(-gamma * lam)], deltas[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo():\n",
    "    env = gym.make('MountainCar-v0')\n",
    "    obs_space = env.observation_space\n",
    "    act_space = env.action_space\n",
    "    \n",
    "    actor_critic = ActorCritic(obs_space, act_space)\n",
    "    \n",
    "    obs_buf, action_buf, advantage_buf, val_buf, reward_buf, logp_buf = [], [], [], [], [], []\n",
    "    \n",
    "    pi_optim = Adam(actor_critic.pi.parameters(), lr=pi_lr)\n",
    "    v_optim = Adam(actor_critic.v.parameters(), lr=vf_lr)\n",
    "    \n",
    "    def ppo_pi_loss(obs, action, advantage, old_logp):\n",
    "        pi, logp = actor_critic.pi(obs, action)\n",
    "        pi_ratio = torch.exp(logp - old_logp)\n",
    "        clip = torch.clamp(pi_ratio, 1 - pi_ratio, 1 + pi_ratio)\n",
    "        pi_loss = -torch.min(pi_ratio * advantage, clip * advantage).mean()\n",
    "        return pi_loss\n",
    "    \n",
    "    def ppo_v_loss(obs, reward):\n",
    "        return (actor_critic.v(obs) - reward).pow(2).mean()\n",
    "    \n",
    "    def ppo_update():\n",
    "        advantage = gae(reward_buf, val_buf)\n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optim.zero_grad()\n",
    "            pi_loss = ppo_pi_loss(obs, action, advantage, old_logp)\n",
    "            pi_loss.backward()\n",
    "            pi_optim.step()\n",
    "            \n",
    "        for i in range(train_v_iters):\n",
    "            v_optim.zero_grad()\n",
    "            v_loss = ppo_v_loss(obs, reward)\n",
    "            v_loss.backward()\n",
    "            v_optim.step()\n",
    "\n",
    "    obs, ep_reward, ep_len = env.reset(), 0, 0\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            action, val, logp_a = actor_critic.step(torch.FloatTensor(obs))\n",
    "            \n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_len += 1\n",
    "            \n",
    "            obs_buf.append(obs)\n",
    "            action_buf.append(action)\n",
    "            reward_buf.append(reward)\n",
    "            val_buf.append(val)\n",
    "            logp_buf.append(logp_a)\n",
    "            \n",
    "            obs = new_obs\n",
    "            done = done or (ep_len == max_ep_len)\n",
    "            if done:\n",
    "                obs, ep_reward, ep_len = env.reset(), 0, 0\n",
    "                break\n",
    "            \n",
    "        ppo_update()\n",
    "                \n",
    "    \n",
    "ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
