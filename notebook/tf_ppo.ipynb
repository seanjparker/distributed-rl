{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpi_proc_id():\n",
    "    return MPI.COMM_WORLD.Get_rank()\n",
    "\n",
    "def mpi_num_procs():\n",
    "    return MPI.COMM_WORLD.Get_size()\n",
    "\n",
    "def mpi_avg(x):\n",
    "    x, scalar = ([x], True) if np.isscalar(x) else (x, False)\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    buff = np.zeros_like(x, dtype=np.float32)\n",
    "    MPI.COMM_WORLD.Allreduce(x, buff, op=MPI.SUM)\n",
    "    return (buff[0] if scalar else buff) / mpi_num_procs()\n",
    "\n",
    "def mpi_avg_grads(module):\n",
    "    if num_procs()==1:\n",
    "        return\n",
    "    for p in module.parameters():\n",
    "        p_grad_numpy = p.grad.numpy()   # numpy view of tensor data\n",
    "        avg_p_grad = mpi_avg(p.grad)\n",
    "        p_grad_numpy[:] = avg_p_grad[:]\n",
    "        \n",
    "def mpi_sync_params(module):\n",
    "    if num_procs()==1:\n",
    "        return\n",
    "    for p in module.parameters():\n",
    "        MPI.COMM_WORLD.Bcast(p.data.numpy(), root=0)\n",
    "        \n",
    "def mpi_fork(n):\n",
    "    if n <= 1:\n",
    "        return\n",
    "    if os.getenv(\"IN_MPI\") is None:\n",
    "        env = os.environ.copy()\n",
    "        env.update(MKL_NUM_THREADS='1', OMP_NUM_THREADS='1', IN_MPI='1')\n",
    "        args = ['mpirun', '-np', str(n)]\n",
    "        args += [sys.executable] + sys.argv\n",
    "        print(args)\n",
    "        subprocess.check_call(args, env=env)\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dimensions, act_dimensions):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dimensions, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, act_dimensions),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def get_distribution(self, obs):\n",
    "        return Categorical(logits=self.actor(obs))\n",
    "    \n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions\n",
    "        pi = self.get_distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = pi.log_prob(act)\n",
    "        return pi, logp_a\n",
    "        \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dimensions):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_dimensions, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.critic(obs), -1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, obs_space, act_space):\n",
    "        super().__init__()\n",
    "        \n",
    "        obs_dimensions = obs_space.shape[0]\n",
    "        \n",
    "        # policy network\n",
    "        self.pi = Actor(obs_dimensions, act_space.n)\n",
    "        \n",
    "        # value network\n",
    "        self.v = Critic(obs_dimensions)\n",
    "        \n",
    "        \n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi.get_distribution(obs)\n",
    "            action = pi.sample()\n",
    "            logp_a = pi.log_prob(action)\n",
    "            val = self.v(obs)\n",
    "\n",
    "        return action.numpy(), val.numpy(), logp_a.numpy()\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        return self.step(obs)[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter definitions\n",
    "num_workers = 1\n",
    "pi_lr = 3e-4\n",
    "v_lr = 1e-3\n",
    "epochs = 50 # 50\n",
    "steps_per_epoch = 4000 // num_workers\n",
    "max_ep_len = 1000\n",
    "train_pi_iters = 80\n",
    "train_v_iters = 80\n",
    "gamma = 0.99\n",
    "lam = 0.97\n",
    "clip_ratio_offset = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    return signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo():\n",
    "    env = gym.make('LunarLander-v2')\n",
    "    obs_space = env.observation_space\n",
    "    act_space = env.action_space\n",
    "\n",
    "    actor_critic = ActorCritic(obs_space, act_space)\n",
    "    \n",
    "    mpi_sync_params(actor_critic)\n",
    "    \n",
    "    obs_buf  = np.zeros((steps_per_epoch, obs_space.shape[0]), dtype=np.float32)\n",
    "    act_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    adv_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    rew_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    rew_boot_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    val_buf  = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    logp_buf = np.zeros(steps_per_epoch, dtype=np.float32)\n",
    "    \n",
    "    pi_optim = Adam(actor_critic.pi.parameters(), lr=pi_lr)\n",
    "    v_optim = Adam(actor_critic.v.parameters(), lr=v_lr)\n",
    "    \n",
    "    def ppo_pi_loss(data):\n",
    "        obs, action, advantage, old_logp = data['obs'], data['act'], data['adv'], data['logp']\n",
    "        \n",
    "        pi, logp = actor_critic.pi(obs, action)\n",
    "        pi_ratio = torch.exp(logp - old_logp)\n",
    "        clipped_adv = torch.clamp(pi_ratio, 1 - clip_ratio_offset, 1 + clip_ratio_offset) * advantage\n",
    "        pi_loss = -(torch.min(pi_ratio * advantage, clipped_adv)).mean()\n",
    "        return pi_loss\n",
    "    \n",
    "    def ppo_v_loss(data):\n",
    "        obs, adj_rewards = data['obs'], data['rew']\n",
    "        return (actor_critic.v(obs) - adj_rewards).pow(2).mean()\n",
    "    \n",
    "    def ppo_update():\n",
    "        data = {k: torch.as_tensor(v, dtype=torch.float32)\n",
    "                for k, v in dict(obs=obs_buf, act=act_buf, rew=rew_boot_buf, adv=adv_buf, logp=logp_buf).items()}\n",
    "        \n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optim.zero_grad()\n",
    "            pi_loss = ppo_pi_loss(data)\n",
    "            pi_loss.backward()\n",
    "            mpi_avg_grads(actor_critic.pi)\n",
    "            pi_optim.step()\n",
    "            \n",
    "        for i in range(train_v_iters):\n",
    "            v_optim.zero_grad()\n",
    "            v_loss = ppo_v_loss(data)\n",
    "            v_loss.backward()\n",
    "            mpi_avg_grads(actor_critic.v)\n",
    "            v_optim.step()\n",
    "\n",
    "    obs, ep_reward, ep_len = env.reset(), 0, 0\n",
    "    ep_reward_history = np.zeros(epochs, dtype=np.float32)\n",
    "    for ep in range(epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            action, val, logp_a = actor_critic.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "            \n",
    "            new_obs, reward, done, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            ep_len += 1\n",
    "            \n",
    "            # buf.store(obs, action, reward, val, logp_a)\n",
    "            obs_buf[t] = obs\n",
    "            act_buf[t] = action\n",
    "            rew_buf[t] = reward\n",
    "            val_buf[t] = val\n",
    "            logp_buf[t] = logp_a\n",
    "            \n",
    "            obs = new_obs\n",
    "            \n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = done or timeout\n",
    "            epoch_ended = t==steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if timeout or epoch_ended:\n",
    "                    _, bootstrap_v, _ = actor_critic.step(torch.as_tensor(obs, dtype=torch.float32))\n",
    "                else:\n",
    "                    bootstrap_v = 0\n",
    "\n",
    "                rews_aug = np.append(rew_buf[:], bootstrap_v)\n",
    "                vals_aug = np.append(val_buf[:], bootstrap_v)\n",
    "                \n",
    "                # GAE-Lambda advantage calculation\n",
    "                gae_deltas = rews_aug[:-1] + gamma * vals_aug[1:] - vals_aug[:-1]\n",
    "                adv_buf[:] = discount_cumsum(gae_deltas, gamma * lam)\n",
    "        \n",
    "                # Computes rewards-to-go, to be targets for the value function\n",
    "                rew_boot_buf[:] = discount_cumsum(rews_aug, gamma)[:-1]\n",
    "\n",
    "                ep_reward_history[ep] = max(ep_reward_history[ep], ep_reward)\n",
    "                obs, ep_reward, done = env.reset(), 0, 0\n",
    "        \n",
    "        ppo_update()\n",
    "        if mpi_proc_id() == 0:\n",
    "            print(f'epoch: {ep + 1}, max reward: {ep_reward_history[ep]:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
